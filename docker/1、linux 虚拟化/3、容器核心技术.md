# 容器核心技术

容器是通过 cgroup 资源限制 以及 namespace + unionfs + 容器引擎 

- cgroup 就是我们常说的资源控制
- namespace：访问隔离
- rootfs： 文件系统隔离。镜像的本质就是一个rootfs文件 
- 容器引擎：生命周期控制



## 简介

这里我们对上面提到的那几种名词一一做个介绍



### Cgroup

 Cgroup 是 Control group 的简称，是 Linux 内核提供的一个特性，用于限制和隔离一组进程对系统资源的使用。对不同资源的具体管理是由各个子系统分工完成的。  Cgroups提供了以下四大功能: 

- 资源限制（Resource Limitation）：cgroups可以对进程组使用的资源总额进行限制。如设定应用运行时使用内存的上限，一旦超过这个配额就发出OOM（Out of Memory）。
- 优先级分配（Prioritization）：通过分配的CPU时间片数量及硬盘IO带宽大小，实际上就相当于控制了进程运行的优先级。
- 资源统计（Accounting）： cgroups可以统计系统的资源使用量，如CPU使用时长、内存用量等等，这个功能非常适用于计费。
- 进程控制（Control）：cgroups可以对进程组执行挂起、恢复等操作。

#### Cgroups中的三个组件

- cgroup 控制组 。cgroup 是对进程分组管理的一种机制，一个cgroup包含一组进程，并可以在这个cgroup上增加Linux subsystem的各种参数的配置，将一组进程和一组subsystem的系统参数关联起来。
- subsystem 子系统。subsystem 是一组资源控制的模块。这块在下面会详细介绍。
- hierarchy 层级树。hierarchy 的功能是把一组cgroup串成一个树状的结构，一个这样的树便是一个hierarchy，通过这种树状的结构，Cgroups可以做到继承。比如我的系统对一组定时的任务进程通过cgroup1限制了CPU的使用率，然后其中有一个定时dump日志的进程还需要限制磁盘IO，为了避免限制了影响到其他进程，就可以创建cgroup2继承于cgroup1并限制磁盘的IO，这样cgroup2便继承了cgroup1中的CPU的限制，并且又增加了磁盘IO的限制而不影响到cgroup1中的其他进程。

### cgroups子系统

cgroup中实现的子系统 (目录位于/sys/fs/cgroup/) 及其作用如下：

| 子系统     | 作用                                     |
| ---------- | ---------------------------------------- |
| devices    | 设备权限控制                             |
| cpuset     | 分配指定的CPU和内存节点                  |
| CPU        | 控制CPU使用率                            |
| cpuacct    | 统计CPU使用情况                          |
| memory     | 限制内存的使用上限                       |
| freezer    | 暂停Cgroup 中的进程                      |
| net_cls    | 配合流控限制网络带宽                     |
| net_prio   | 设置进程的网络流量优先级                 |
| perf_event | 允许 Perf 工具基于 Cgroup 分组做性能检测 |
| huge_tlb   | 限制 HugeTLB 的使用                      |

每个子系统的目录下有更详细的设置项，例如：**cpu**

```sh
[root@c001 bin]# ls /sys/fs/cgroup/cpu
cgroup.clone_children  cgroup.procs          cpuacct.stat   cpuacct.usage_percpu  cpu.cfs_quota_us  cpu.rt_runtime_us  cpu.stat  kubepods           release_agent  tasks
cgroup.event_control   cgroup.sane_behavior  cpuacct.usage  cpu.cfs_period_us     cpu.rt_period_us  cpu.shares         docker    notify_on_release  system.slice   user.slice
```

我们可以看到子系统目录下有更详细的限制配置项，请根据实体情况进行配置。

<br/>

在 Cgroup 出现之前，只能对一个进程做资源限制，如 `ulimit` 限制一个进程的打开文件上限、栈大小。而 Cgroup 可以对进程进行任意分组，如何分组由用户自定义。

1、cpu 子系统

作用：cpu 子系统用于限制进程的 CPU 利用率。具体支持三个功能

- CPU 比重分配。使用 cpu.shares 接口。
- CPU 带宽限制。使用 cpu.cfs_period_us 和 cpu.cfs_quota_us 接口。
-  实时进程的 CPU 带宽限制。使用 cpu_rt_period_us 和 cpu_rt_quota_us 接口。

2、cpuacct 子系统

作用：统计各个 Cgroup 的 CPU 使用情况，有如下接口：

- cpuacct.stat: 报告这个 Cgroup 在用户态和内核态消耗的 CPU 时间，单位是 赫兹。
- cpuacct.usage： 报告该 Cgroup 消耗的总 CPU 时间。
- cpuacct.usage_percpu：报告该 Cgroup 在每个 CPU 上的消耗时间。

3、memory 子系统

作用：限制 Cgroup 所能使用的内存上限。

- memory.limit_in_bytes：设定内存上限，单位字节。
   默认情况下，如果使用的内存超过上限，Linux 内核会试图回收内存，如果这样仍无法将内存降到限制的范围内，就会触发 OOM，选择杀死该Cgroup 中的某个进程。
- memory.memsw,limit_in_bytes: 设定内存加上交换内存区的总量。
- memory.oom_control： 如果设置为0，那么内存超过上限时，不会杀死进程，而是阻塞等待进程释放内存；同时系统会向用户态发送事件通知。
- memory.stat: 报告内存使用信息。

这里专门讲一下监控和统计相关的参数，比如cAdvisor采集的那些参数。

- memory.usage_bytes：报告该 cgroup中进程使用的当前总内存用量（以字节为单位）。
- memory.max_usage_bytes：报告该 cgroup 中进程使用的最大内存用量。



4、blkio限制 Cgroup 对 阻塞 IO 的使用。

- blkio.weight: 设置权值，范围在[100, 1000]，属于比重分配，不是绝对带宽。因此只有当不同 Cgroup 争用同一个 阻塞设备时才起作用
- blkio.weight_device： 对具体设备设置权值。它会覆盖上面的选项值。
- blkio.throttle.read_bps_device: 对具体的设备，设置每秒读磁盘的带宽上限。
- blkio.throttle.write_bps_device: 对具体的设备，设置每秒写磁盘的带宽上限。
- blkio.throttle.read_iops_device: 对具体的设备，设置每秒读磁盘的IOPS带宽上限。
- blkio.throttle.write_iops_device: 对具体的设备，设置每秒写磁盘的IOPS带宽上限。

5、devices 子系统

作用：控制 Cgroup 的进程对哪些设备有访问权限

- devices.list: 只读文件，显示目前允许被访问的设备列表，文件格式为
   `类型[a|b|c] 设备号[major:minor] 权限[r/w/m 的组合]`
   a/b/c 表示 所有设备、块设备和字符设备。
- devices.allow： 只写文件，以上述格式描述允许相应设备的访问列表。
- devices.deny： 只写文件，以上述格式描述禁止相应设备的访问列表

扩展：https://www.wumingx.com/k8s/docker-cgroup-limit.html



### Namespace

Namespace 是将内核的全局资源做封装，使得每个namespace 都有一份独立的资源，因此不同的进程在各自的namespace内对同一种资源的使用互不干扰。
 举个例子，执行`sethostname`这个系统调用会改变主机名，这个主机名就是全局资源，内核通过 **UTS Namespace**可以将不同的进程分隔在不同的 UTS Namespace 中，在某个 Namespace 修改主机名时，另一个 Namespace 的主机名保持不变。

目前，Linux 内核实现了6种 Namespace。

| Namespace | 作用                                |
| --------- | ----------------------------------- |
| IPC       | 隔离 System V IPC 和 POSIX 消息队列 |
| Network   | 隔离网络资源                        |
| Mount     | 隔离文件系统挂载点                  |
| PID       | 隔离进程ID                          |
| UTS       | 隔离主机名和域名                    |
| User      | 隔离用户和用户组                    |

与命名空间相关的三个系统调用：

- `clone`创建全新的Namespace，由clone创建的新进程就位于这个新的namespace里。创建时传入 flags参数，可选值有 以下六种，分别对应上面六种namespace。
  - CLONE_NEWIPC
  -  CLONE_NEWNET
  - CLONE_NEWNS 
  - CLONE_NEWPID
  - CLONE_NEWUTS
  - CLONE_NEWUSER
- `unshare`为已有进程创建新的namespace，并且使当前进程脱离某个namespace。
- `setns`把某个进程放在已有的某个namespace里。

下面我们来看下linux的6种namespace，或者我们可以在 /proc/xxpid/ns/ 目录下看到当前进程所使用的 namspace

**6种命名空间**

**1、UTS namespace**
 UTS（UNIX Time-sharing System）namespace提供了主机名和域名的隔离，这样每个Docker容器就可以拥有独立的主机名和域名了，在网络上可以被视为一个独立的节点，而非宿主机上的一个进程。Docker中，每个镜像基本都以自身所提供的服务名称来命名镜像的hostname，且不会对宿主机产生任何影响，其原理就是利用了UTS namespace 

**2、IPC namespace**
 进程间通信（Inter-Process Communication，IPC）涉及的IPC资源包括常见的信号量、消息队列和共享内存。申请IPC资源就申请了一个全局唯一的32位ID，所以IPC namespace中实际上包含了系统IPC标识符以及实现POSIX消息队列的文件系统。在同一个IPC namespace下的进程彼此可见，不同IPC namespace下的进程则互相不可见。 

**3、PID namespace**

PID namespace隔离非常实用，对进程PID重新标号，即两个不同 namespace的进程可以有相同的PID。每个 PID namespace都有自己的计数程序。内核为所有的 PID namespace维护了一个树状结构，最顶层的是系统初始时创建的，被称为 root namespace它创建的新 PID namespace被称为 child namespace（树的子节点），而原先的 PID namespace就是新创建的 namespace的parent namespace（树的父节点）通过这种方式，不同的 PID namespaces会成一个层级体系。所属的父节点可以看到子节点中的进程，并可以通过信号等方式对子节点中的进程产生影响。反过来，子节点却不能看到父节点 PID namespace中的任何内容，由此产生如下结论

1. 每个 PID namespace中的第一个进程“PID1”，都会像传统 Linux中的init进程一样拥有特权，起特殊作用。
2. 一个 namespace中的进程，不可能通过kill或 ptrace影响父节点或者兄弟节点中的进程，因为其他节点的PID在这 namespace个中没有任何意义。
3. 如果你在新的 PID namespace中重新挂载/proc文件系统，会发现其下只显示同属一个pid namespace中的其他进程。
4. 在 root namespace中可以看到所有的进程，并且递归包含所有子节点中的进程。

到这里，读者可能已经联想到一种在外部监控Docker中运行程序的方法了，就是监控Docker daemon所在的PID namespace下的所有进程及其子进程，再进行筛选即可。

**4、Mount namespace**

隔离文件挂载点，每个进程能看到的文件系统都记录在`/proc/$$/mounts`里。在一个 namespace 里挂载、卸载的动作不会影响到其他 namespace。

**5、Network namespace**

当我们了解完各类 namespace，兴致勃勃地构建出一个容器，并在容器中启动一个 Apache进程时，却出现了“80端口已被占用”的错误，原来主机上已经运行了一个 Apache进程，这时就要借助 Network namespace技术进行网络隔离。

network namespace主要提供了关于网络资源的隔离，包括网络设备、IPv4和IPv6协议栈、IP路由表、防火墙、/proc/net目录、/sys/ Class/net目录、套接字( socket)等。一个物理的网络设备最多存在于一个 network namespace中，可以通过创建 veth pair(虚拟网络设备对：有两端类似管道，如果数据从一端传入另一端也能接收到，反之亦然)在不同的 network namespace间创建通道，以达到通信目的。

一般情况下，物理网络设备都分配在最初的 root namespace(表示系统默认的 namespace)中。但是如果有多块物理网卡，也可以把其中一块或多块分配给新创建的 network namespace。需要注意的是，当新创建的 network namespace被释放时(所有内部的进程都终止并且 namespace文件没 有被挂载或打开)，在这个 namespace中的物理网卡会返回到 root namespace，而非创建该进程的父 进程所在的 network namespace 。

当说到 network namespace时，指的未必是真正的网络隔离,而是把网络独立出来，给外部用户一种透明的感觉，仿佛在与一个独立网络实体进行通信。为了达到该目的，容器的经典做法就是创建一个 veth pair，一端放置在新的 namespace中,通常命名为eth0，一端放在原先的 namespace 中连接物理网络设备，再通过把多个设备接入网桥或者进行路由转发，来实现通信的目的。

也许读者会好奇，在建立起 veth pair之前，新旧 namespace该如何通信呢？答案是pipe(管道)。以 Docker daemon，启动容器的过程为例，假设容器内初始化的进程称为 init docker daemon在宿主机上负责创建这个 veth pair，把一端绑定到 docker网桥上，另一端接入新建的 network namespace进程中。这个过程执行期间，Docker daemon和init就通过pipe进行通信。具体来说，就是在 Docker daemon完成 veth pair的创建之前，init在管道的另一端循环等待，直到管道另一端传来 Dockerdaemon关于veth设备的信息，并关闭管道。init才结束等待的过程，并把它的“eth0”启动起来。

**6、User namespace**
 user namespace主要隔离了安全相关的标识符( identifier)和属性( attribute)，包括用户ID用户组D、root目录、key(指**)以及特殊权限。通俗地讲，一个普通用户的进程通过 clone()创建的新进程在新 user namespace中可以拥有不同的用户和用户组。这意味着一个进程在容器外属于一个没有特权的普通用户，但是它创建的容器进程却属于拥有所有权限的超级用户，这个技术为容器提供了极大的自由。 

扩展：https://www.wumingx.com/k8s/docker-namespace.html







### rootfs

 rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 

rootfs 挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。

容器的rootfs由三部分组成，1：只读层、2：可读写层、3：init层

1.只读层:都以增量的方式分别包含了  操作系统的一部分。

2.可读写：就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。

3.Init 层：是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。

 根文件系统首先是内核启动时所mount的第一个文件系统，内核代码映像文件保存在根文件系统中，而系统引导启动程序会在根文件系统挂载之后从中把一些基本的初始化脚本和服务等加载到内存中去运行。 







